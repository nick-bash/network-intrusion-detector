{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dfbd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pandas as pd\n",
    "import time\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9daff483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 21.8242244720459 seconds\n"
     ]
    }
   ],
   "source": [
    "# LOAD FILES INTO DATAFRAME\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "# Local path - prefix string with r to avoid unicode escape on '\\U'\n",
    "# path = r'C:\\Users\\Nick Bashour\\Documents\\Personal\\14. Stanford\\2. Academics\\3. 2021 Spring\\1. CS 230\\2. Project\\2. Data\\MachineLearningCVE\\\\'\n",
    "# AWS EC2 path\n",
    "path = 'data/'\n",
    "\n",
    "files = [\n",
    "    'Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',\n",
    "    'Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
    "    'Monday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n",
    "    'Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n",
    "    'Tuesday-WorkingHours.pcap_ISCX.csv',\n",
    "    'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "]\n",
    "\n",
    "pd_files = {} # empty dict of pd objects\n",
    "\n",
    "# Read in every file\n",
    "for i in range(len(files)):    \n",
    "    files[i] = path + files[i]   \n",
    "    pd_files[i] = pd.read_csv(files[i])\n",
    "    pd_files[i][\"File #\"] = i  # a column to track where each file came from\n",
    "    \n",
    "df = pd.concat(pd_files)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Data loaded in\", toc-tic, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e084d6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are:\n",
      " ['BENIGN' 'Bot' 'DDoS' 'DoS GoldenEye' 'DoS Hulk' 'DoS Slowhttptest'\n",
      " 'DoS slowloris' 'FTP-Patator' 'Heartbleed' 'Infiltration' 'PortScan'\n",
      " 'SSH-Patator' 'Web Attack - Brute Force' 'Web Attack - Sql Injection'\n",
      " 'Web Attack - XSS']\n",
      "\n",
      "# of unique labels:  15\n"
     ]
    }
   ],
   "source": [
    "# EXTRACT LABELS\n",
    "labels = df['Label'].unique()\n",
    "labels = np.sort(labels)\n",
    "print(\"Labels are:\\n\", labels)\n",
    "print(\"\\n# of unique labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enabling-flight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "BENIGN                        2273097\n",
      "Bot                              1966\n",
      "DDoS                           128027\n",
      "DoS GoldenEye                   10293\n",
      "DoS Hulk                       231073\n",
      "DoS Slowhttptest                 5499\n",
      "DoS slowloris                    5796\n",
      "FTP-Patator                      7938\n",
      "Heartbleed                         11\n",
      "Infiltration                       36\n",
      "PortScan                       158930\n",
      "SSH-Patator                      5897\n",
      "Web Attack - Brute Force         1507\n",
      "Web Attack - Sql Injection         21\n",
      "Web Attack - XSS                  652\n",
      "dtype: int64\n",
      "File #\n",
      "0    225745\n",
      "1    286467\n",
      "2    191033\n",
      "3    529918\n",
      "4    288602\n",
      "5    170366\n",
      "6    445909\n",
      "7    692703\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CAPTURE LABEL AND FILE FREQUENCY\n",
    "label_freq = df.groupby(\"Label\").size()\n",
    "print(label_freq)\n",
    "\n",
    "file_freq = df.groupby(\"File #\").size()\n",
    "print(file_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "quiet-staff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lables to indices is:\n",
      " {'BENIGN': 0, 'Bot': 1, 'DDoS': 2, 'DoS GoldenEye': 3, 'DoS Hulk': 4, 'DoS Slowhttptest': 5, 'DoS slowloris': 6, 'FTP-Patator': 7, 'Heartbleed': 8, 'Infiltration': 9, 'PortScan': 10, 'SSH-Patator': 11, 'Web Attack - Brute Force': 12, 'Web Attack - Sql Injection': 13, 'Web Attack - XSS': 14} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CONVERT DATAFRAME LABELS INTO INDICES FOR ONE-HOT REPRESENTATION\n",
    "\n",
    "# Create a dictionary mapping labels to indices\n",
    "labels_to_indices = {}\n",
    "for i in range(len(labels)):\n",
    "    labels_to_indices[labels[i]] = i\n",
    "print(\"Lables to indices is:\\n\", labels_to_indices, \"\\n\")\n",
    "\n",
    "# Convert the dataframe column to indices\n",
    "df[\"Label\"] = df[\"Label\"].map(labels_to_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "indirect-asthma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "0     2273097\n",
      "1        1966\n",
      "2      128027\n",
      "3       10293\n",
      "4      231073\n",
      "5        5499\n",
      "6        5796\n",
      "7        7938\n",
      "8          11\n",
      "9          36\n",
      "10     158930\n",
      "11       5897\n",
      "12       1507\n",
      "13         21\n",
      "14        652\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CHECK INDEX / LABEL MAPPING WORKED\n",
    "index_freq = df.groupby(\"Label\").size()\n",
    "print(index_freq)\n",
    "for i in range(len(labels)):\n",
    "    assert(label_freq[i] == index_freq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "virtual-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 70.18049430847168 seconds\n"
     ]
    }
   ],
   "source": [
    "# CREATE TEST & TRAINING DATA: iterate through each label type. Place 1,000 of each label type\n",
    "# into the test file and the remaining into training.\n",
    "tic = time.time()\n",
    "\n",
    "test_samples_per_label = 1000\n",
    "\n",
    "# Select features\n",
    "feature_cols = df.columns.tolist()\n",
    "feature_cols.remove('Label')\n",
    "feature_cols.remove('File #')\n",
    "feature_cols.remove('Destination Port')\n",
    "\n",
    "# Empty dataframes to store test and training variables\n",
    "X_train = pd.DataFrame()\n",
    "X_test = pd.DataFrame()\n",
    "\n",
    "# Iterate through files\n",
    "for i in range(len(labels)):\n",
    "    data = df[df[\"Label\"] == i]\n",
    "    data = data.sample(frac=1) # random sort\n",
    "\n",
    "    # Add the first 1000 (or as many as available) datapoints to test, remaining to train\n",
    "    test_rows = min(test_samples_per_label, len(data))\n",
    "    test = data.head(test_rows)\n",
    "    train = data.tail(len(data) - test_rows)\n",
    "    assert(len(test) + len(train) == len(data)) \n",
    "        \n",
    "    # Append X features and labels to train / test sets\n",
    "    X_train = X_train.append(train[feature_cols])\n",
    "    X_test = X_test.append(test[feature_cols])\n",
    "    if i == 0:\n",
    "        Y_train = train['Label']\n",
    "        Y_test = test['Label']\n",
    "    else:\n",
    "        Y_train = Y_train.append(train['Label'])\n",
    "        Y_test = Y_test.append(test['Label'])\n",
    "    \n",
    "# Transpose labels so their shape is (# samples, 1)\n",
    "Y_train = Y_train.transpose()\n",
    "Y_test = Y_test.transpose()\n",
    "                      \n",
    "toc = time.time()\n",
    "print(\"Time elapsed:\", toc-tic, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "helpful-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check # of test samples corresponds to sum of min(1000,# of samples) across labels\n",
    "n_test_samples = 0\n",
    "for i in range(len(labels)):\n",
    "    n_test_samples += min(1000, index_freq[i])\n",
    "assert n_test_samples == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wanted-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2819023, 77)\n",
      "# of Nan's: 1355\n",
      "<class 'numpy.ndarray'>\n",
      "(11720, 77)\n",
      "# of Nan's: 3\n",
      "<class 'numpy.ndarray'>\n",
      "(2819023,)\n",
      "# of Nan's: 0\n",
      "<class 'numpy.ndarray'>\n",
      "(11720,)\n",
      "# of Nan's: 0\n"
     ]
    }
   ],
   "source": [
    "# CONVERT PANDAS INTO NP ARRAYS AND RUN CHECKS\n",
    "X_train, X_test, Y_train, Y_test = X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy(), Y_test.to_numpy()\n",
    "\n",
    "for i in [X_train, X_test, Y_train, Y_test]:\n",
    "    print(type(i))\n",
    "    print(i.shape)    \n",
    "    # Replace NaN's with 0's\n",
    "    print(\"# of Nan's:\", np.count_nonzero(np.isnan(i)))\n",
    "    i[np.isnan(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dramatic-vinyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2819023, 15)\n",
      "(11720, 15)\n"
     ]
    }
   ],
   "source": [
    "# CONVERT Y TO ONE-HOT ARRAYS\n",
    "\n",
    "def one_hot(y, classes):\n",
    "    # Input: array y of shape (m,) or (m,1) whose values represent indices\n",
    "    # Output: one-hot matrix of shape (m, classes)\n",
    "    y_OH = np.zeros((len(y), classes))\n",
    "    for i in range(len(y)):\n",
    "        y_OH[i,int(y[i])] = 1\n",
    "    return y_OH\n",
    "\n",
    "Y_train = one_hot(Y_train, len(labels))\n",
    "Y_test = one_hot(Y_test, len(labels))\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4a3b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in subtract\n",
      "  if __name__ == '__main__':\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in subtract\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# Normalize training & test sets according to training set data\n",
    "mean = np.mean(X_train, axis=0)\n",
    "epsilon = pow(10,-7)\n",
    "std = np.std(X_train, axis=0) + epsilon\n",
    "mean = mean.reshape(1, mean.shape[0])\n",
    "std = std.reshape(1, std.shape[0])\n",
    "assert(X_train.shape[1] == mean.shape[1]) # ensure proper sizes\n",
    "assert(X_train.shape[1] == std.shape[1])\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "steady-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2819023, 77)\n",
      "# of Nan's: 5638046\n",
      "(11720, 77)\n",
      "# of Nan's: 23440\n"
     ]
    }
   ],
   "source": [
    "# Check again for NaN's and remove\n",
    "for i in [X_train, X_test]:\n",
    "    print(i.shape)\n",
    "    \n",
    "    # Replace NaN's with 0's\n",
    "    print(\"# of Nan's:\", np.count_nonzero(np.isnan(i)))\n",
    "    i[np.isnan(i)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3336571e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data output in 252.58813571929932 seconds\n"
     ]
    }
   ],
   "source": [
    "# Write files to CSV\n",
    "tic = time.time()\n",
    "\n",
    "path = \"clean_data/\"\n",
    "out_X_train = open(path+\"X_train.csv\", \"w\")\n",
    "np.savetxt(path+'X_train.csv', X_train, delimiter=',')\n",
    "out_Y_train = open(path+\"Y_train.csv\", \"w\")\n",
    "np.savetxt(path+'Y_train.csv', Y_train, delimiter=',')\n",
    "\n",
    "out_X_test = open(path+\"X_test.csv\", \"w\")\n",
    "np.savetxt(path+'X_test.csv', X_test, delimiter=',')\n",
    "out_Y_test = open(path+\"Y_test.csv\", \"w\")\n",
    "np.savetxt(path+'Y_test.csv', Y_test, delimiter=',')\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Data output in\", toc-tic, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
