{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T10:39:11.217240Z",
     "iopub.status.busy": "2021-05-19T10:39:11.216629Z",
     "iopub.status.idle": "2021-05-19T10:39:18.231161Z",
     "shell.execute_reply": "2021-05-19T10:39:18.231563Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 1.231572151184082 sec's\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "tic = time.time()\n",
    "\n",
    "# For local machine:\n",
    "# path = r'C:\\Users\\Nick Bashour\\Documents\\Personal\\14. Stanford\\2. Academics\\3. 2021 Spring\\1. CS 230\\2. Project\\3. Code\\\\'\n",
    "# For AWS EC2 instance:\n",
    "path = \"clean_data/\"\n",
    "\n",
    "X_test = np.genfromtxt(str(path+'X_test.csv'), delimiter=',')\n",
    "Y_test = np.genfromtxt(str(path+'Y_test.csv'), delimiter=',')\n",
    "\n",
    "toc = time.time()\n",
    "print(\"time elapsed: \" + str(toc-tic) + \" sec's\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shapes: \n",
      "   X: (11720, 77)\n",
      "   Y: (11720, 15)\n",
      "# of X features: 77\n",
      "# of Y labels: 15\n"
     ]
    }
   ],
   "source": [
    "# Data dimensions\n",
    "print(\"Test set shapes: \")\n",
    "print(\"   X: \" + str(X_test.shape))\n",
    "print(\"   Y: \" + str(Y_test.shape))\n",
    "# print(np.count_nonzero(np.isnan(X_train)))\n",
    "\n",
    "nx = X_test.shape[1]\n",
    "ny = Y_test.shape[1]\n",
    "print(\"# of X features:\",nx)\n",
    "print(\"# of Y labels:\", ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label count: 15\n"
     ]
    }
   ],
   "source": [
    "# Create labels\n",
    "labels = np.array(['BENIGN', 'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'DoS Slowhttptest',\n",
    " 'DoS slowloris', 'FTP-Patator', 'Heartbleed', 'Infiltration', 'PortScan',\n",
    " 'SSH-Patator', 'Web Attack - Brute Force', 'Web Attack - Sql Injection',\n",
    " 'Web Attack - XSS'], dtype=object)\n",
    "\n",
    "print(\"Unique label count: \"+ str(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to calculate, by label type\n",
    "metrics_cols = [\"Model_Name\", \"Runtime\"]\n",
    "for l in labels:\n",
    "    metrics_cols.append(l+\" TP\")\n",
    "    metrics_cols.append(l+\" FP\")\n",
    "    metrics_cols.append(l+\" TN\")\n",
    "    metrics_cols.append(l+\" FN\")\n",
    "    \n",
    "# Store metrics in a pandas dataframe\n",
    "metrics = pd.DataFrame(columns = metrics_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort arrays a and b (with same first dimension length) according to a sorted version of the sorter\n",
    "def sort_by_sorter(sorter, a, b):\n",
    "    assert len(sorter) == len(a)\n",
    "    assert len(a) == len(b)    \n",
    "    sorter = np.argsort(sorter) # returns array representing sorted index positions\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    for old_index, new_index in enumerate(sorter):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for analyzing the model\n",
    "\n",
    "def logits_to_OH(y):\n",
    "    # Converts a matrix of shape (classes,) or (samples, classes) from logits to one-hot\n",
    "    # by selecting the highest probability item as the true label\n",
    "    one_hot = np.zeros(y.shape)\n",
    "    \n",
    "    # Shape is (classes,)\n",
    "    if(len(y.shape) == 1):\n",
    "        one_hot[np.argmax(y)] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    # Shape is (samples, classes)\n",
    "    for i in range(y.shape[0]):\n",
    "        j = np.argmax(y[i])\n",
    "        one_hot[i, j] = 1\n",
    "    return one_hot\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, negative_index):    \n",
    "    # Given: true and predicted one-hot matrices of shape (samples, classes)\n",
    "    #        and 'negative' samples defined as those with a 1 at negative_index\n",
    "    # Returns: number of TP, FP, TN, FN\n",
    "    # Note: this function is only meant to work for this application, where\n",
    "    #       'BENIGN' labels are the only 'negatives'. Correctly classifying\n",
    "    #       network activity as not benign, but incorrectly identifying the type of\n",
    "    #       attack, is counted as a true positive.\n",
    "    \n",
    "    assert y_true.shape == y_pred.shape\n",
    "    \n",
    "    TP, FP, TN, FN = 0, 0, 0, 0\n",
    "    for i in range(y_pred.shape[0]):        \n",
    "        # Negative predictions - 'BENIGN'\n",
    "        if (np.argmax(y_pred[i]) == negative_index):\n",
    "            if np.all((y_pred[i] == y_true[i])):\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        # Positive predictions - anything but 'BENIGN'\n",
    "        else:\n",
    "            # Correctly classifying an attack as not benign\n",
    "            # but incorrectly identifying the type of attack\n",
    "            # is still considered a true positive\n",
    "            if (np.argmax(y_true[i]) != negative_index):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    \n",
    "    assert (TP + FP + TN + FN) == y_pred.shape[0]    \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "# Testing\n",
    "true = np.array([[0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "pred = np.array([[1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0]])\n",
    "print(confusion_matrix(true, pred, 0)) # should be 1,0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, sort the test data according to their labels\n",
    "sorter = [i for i in range(len(labels))] # array equal to 0, 1, ... 14\n",
    "sorter = np.dot(Y_test, sorter) # creates a (test_size,1) array to be used for the sort\n",
    "X_test, Y_test = sort_by_sorter(sorter, X_test, Y_test)\n",
    "\n",
    "# Capture the # of samples by label\n",
    "samples_of_label = np.zeros(len(labels))\n",
    "for i in range(len(labels)):\n",
    "    samples_of_label[i] = np.sum(sorter == i)\n",
    "assert np.sum(samples_of_label) == len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210603_4_L_25e.h5 runtime: 0.08439493179321289 seconds\n",
      "210603_4_L_50e.h5 runtime: 0.08257126808166504 seconds\n"
     ]
    }
   ],
   "source": [
    "# Models to analyze\n",
    "models = ['210603_4_L_25e.h5', '210603_4_L_50e.h5']\n",
    "          \n",
    "# Analyze metrics for all models\n",
    "for i in range(len(models)):    \n",
    "    # Load pre-trained model\n",
    "    path = 'saved_models/'\n",
    "    model = load_model(path+models[i])\n",
    "    Y_pred = logits_to_OH(model.predict(X_test))\n",
    "    \n",
    "    metrics.loc[i, \"Model_Name\"] = models[i]\n",
    "    \n",
    "    tic = time.time()   \n",
    "    \n",
    "    # Iterate through every label type\n",
    "    start_index = 0\n",
    "    for j in range(len(labels)):        \n",
    "        end_index = start_index + int(samples_of_label[j])\n",
    "        \n",
    "        # Store confusion matrix in dataframe\n",
    "        TP, FP, TN, FN = confusion_matrix(Y_test[start_index:end_index,:],\n",
    "                                          Y_pred[start_index:end_index,:], 0)\n",
    "        metrics.loc[i,str(labels[j]+\" TP\")] = TP\n",
    "        metrics.loc[i,str(labels[j]+\" FP\")] = FP\n",
    "        metrics.loc[i,str(labels[j]+\" TN\")] = TN\n",
    "        metrics.loc[i,str(labels[j]+\" FN\")] = FN\n",
    "        \n",
    "        start_index += int(samples_of_label[j])\n",
    "        \n",
    "    toc = time.time()    \n",
    "    metrics.loc[i, \"Runtime\"] = toc-tic\n",
    "    print(models[i], \"runtime:\", toc-tic,\"seconds\")\n",
    "    \n",
    "# Run check sums\n",
    "checks = np.zeros((len(models),len(labels)))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(labels)):\n",
    "        checks[i, j] += metrics.loc[0,str(labels[j]+\" TP\")]\n",
    "        checks[i, j] += metrics.loc[0,str(labels[j]+\" FP\")]\n",
    "        checks[i, j] += metrics.loc[0,str(labels[j]+\" TN\")]\n",
    "        checks[i, j] += metrics.loc[0,str(labels[j]+\" FN\")]\n",
    "    assert np.all(checks[i] == samples_of_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics to CSV for further analytics\n",
    "path = \"model_stats/\"\n",
    "name = \"210603_1719_stats.csv\"\n",
    "metrics.to_csv(path+name, index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
